# Data Quality Validation Framework

A comprehensive, configurable data quality validation solution for enterprise data engineering pipelines. Built to automatically detect, document, and resolve common data quality issues that plague production systems.

---

## ğŸ¯ Problem Statement

Poor data quality is one of the leading causes of failed analytics projects and unreliable business decisions. According to industry research, organizations lose an average of **$12.9 million annually** due to poor data quality.

### Common Issues Addressed
- âŒ **Duplicate Records** - Inflate metrics and create processing bottlenecks
- âŒ **Missing Values** - Break downstream pipelines and skew analyses  
- âŒ **Invalid Data Types** - Cause runtime errors in production systems
- âŒ **Out-of-Range Values** - Indicate data entry errors or corruption
- âŒ **Format Inconsistencies** - Emails, phone numbers, dates with varying patterns
- âŒ **Statistical Outliers** - May indicate fraudulent activity or system glitches

### Solution
An intelligent, modular framework that handles all these issues while providing:
- âœ… Automatic detection and resolution strategies
- âœ… Detailed audit logging and quality reports
- âœ… Seamless ETL/ELT pipeline integration
- âœ… Configurable validation rules
- âœ… Quality metrics for monitoring

---

## ğŸš€ Features

### Core Validation Methods

| Feature | Description |
|---------|-------------|
| **Duplicate Detection** | Identifies and removes duplicate records based on specified columns |
| **Null Handling** | Multiple strategies: removal, median/mean imputation, forward-fill, backward-fill |
| **Type Validation** | Automatic type casting with error handling and validation |
| **Range Checking** | Validates numeric values are within specified min/max bounds |
| **Pattern Validation** | Regex-based validation for emails, phone numbers, dates, etc. |
| **Outlier Detection** | Statistical outlier identification using IQR and Z-score methods |
| **Required Columns Check** | Ensures mandatory columns exist in the dataset |

### Logging & Reporting

- **Multi-level Logging**: DEBUG, INFO, WARNING, ERROR levels
- **Dual Output**: Console and file-based logging
- **Quality Reports**: JSON-formatted detailed reports with metrics
- **Audit Trail**: Complete history of all validation operations
- **Metrics Tracking**: Rows processed, rows cleaned, issues found

---

## ğŸ“‹ Requirements

```
pandas>=1.0.0
numpy>=1.18.0
python>=3.7
```

## ğŸ’¾ Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd Capstone
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸ“‚ Project Structure

```
Capstone/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ data_quality_validator.py          # Main validation framework
â”œâ”€â”€ data_quality_documentation.md      # Technical documentation
â”œâ”€â”€ quality_report.json                # Sample output report
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ logs/                              # Validation logs
â””â”€â”€ Screenshots/                       # Visual documentation
```

---

## ğŸ”§ Usage

### Basic Example

```python
import pandas as pd
from data_quality_validator import DataQualityValidator

# Load your data
df = pd.read_csv('data.csv')

# Define validation configuration
config = {
    'check_duplicates': {
        'subset': ['user_id'],
        'keep': 'first'
    },
    'required_columns': ['user_id', 'email', 'age'],
    'null_handling': {
        'age': {'strategy': 'fill_median'},
        'email': {'strategy': 'drop_row'}
    },
    'type_validation': {
        'age': 'int',
        'salary': 'float',
        'created_date': 'datetime'
    },
    'range_checks': {
        'age': {'min': 0, 'max': 120},
        'salary': {'min': 0, 'max': 1000000}
    },
    'pattern_validation': {
        'email': {'pattern': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'},
        'phone': {'pattern': r'^\d{10}$'}
    },
    'outlier_detection': {
        'salary': {'method': 'iqr', 'threshold': 1.5}
    }
}

# Create validator with logging
validator = DataQualityValidator(
    log_file='logs/validation.log',
    log_level='INFO'
)

# Run validation
cleaned_df, quality_report = validator.validate_and_clean(df, config)

# Save results
cleaned_df.to_csv('cleaned_data.csv', index=False)
print(validator.get_report_json())
```

---

## âš™ï¸ Configuration Reference

### Configuration Dictionary Structure

```python
config = {
    # Check for duplicate rows
    'check_duplicates': {
        'subset': ['column1', 'column2'],  # Columns to check
        'keep': 'first'                     # 'first', 'last', or False (remove all)
    },
    
    # Required columns that must exist
    'required_columns': ['col1', 'col2'],
    
    # Handle missing values
    'null_handling': {
        'column_name': {
            'strategy': 'fill_median'       # 'drop_row', 'fill_mean', 'fill_median', 'fill_forward', 'fill_backward'
        }
    },
    
    # Validate/convert data types
    'type_validation': {
        'column_name': 'int'               # 'int', 'float', 'string', 'datetime'
    },
    
    # Check numeric ranges
    'range_checks': {
        'column_name': {'min': 0, 'max': 100}
    },
    
    # Pattern matching with regex
    'pattern_validation': {
        'column_name': {'pattern': r'^[A-Z]'}
    },
    
    # Outlier detection
    'outlier_detection': {
        'column_name': {
            'method': 'iqr',                # 'iqr' or 'zscore'
            'threshold': 1.5                 # IQR multiplier or Z-score threshold
        }
    }
}
```

---

## ğŸ“Š Output & Reports

### Quality Report JSON

The validator generates a comprehensive JSON report:

```json
{
  "timestamp": "2026-01-27T10:55:44.993039",
  "checks_performed": [
    "duplicate_check",
    "required_columns_check",
    "null_handling",
    "type_validation",
    "range_validation",
    "pattern_validation",
    "outlier_detection"
  ],
  "issues_found": [
    {
      "type": "duplicates",
      "count": 1,
      "columns_checked": ["user_id"]
    },
    {
      "type": "null_values",
      "column": "age",
      "count": 1,
      "percentage": 11.11,
      "strategy": "fill_median"
    }
  ],
  "rows_processed": 9,
  "rows_cleaned": 8
}
```

### Logging Output

Detailed logs capture every validation step:

```
2026-01-27 10:55:44 - DataQualityValidator - INFO - ============================================================
2026-01-27 10:55:44 - DataQualityValidator - INFO - Data Quality Validator Initialized
2026-01-27 10:55:44 - DataQualityValidator - INFO - Starting validation pipeline on 9 rows
2026-01-27 10:55:44 - DataQualityValidator - INFO - Running duplicate check...
2026-01-27 10:55:44 - DataQualityValidator - INFO - Handling null values...
2026-01-27 10:55:44 - DataQualityValidator - INFO - VALIDATION PIPELINE COMPLETE
```

---

## ğŸ”„ Integration with ETL Pipelines

### Apache Airflow Example

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from data_quality_validator import DataQualityValidator

def validate_data(**context):
    df = pd.read_csv('raw_data.csv')
    validator = DataQualityValidator(log_file='logs/airflow_validation.log')
    cleaned_df, report = validator.validate_and_clean(df, config)
    cleaned_df.to_parquet('validated_data.parquet')
    return report

with DAG('data_validation_pipeline', ...) as dag:
    validate_task = PythonOperator(
        task_id='validate_data',
        python_callable=validate_data
    )
```

---

## ğŸ› ï¸ Advanced Features

### Custom Validation Strategies

The framework supports extending validation logic:

```python
class CustomValidator(DataQualityValidator):
    def _custom_validation(self, df, config):
        # Add your custom validation logic
        return df
```

### Batch Processing

```python
validator = DataQualityValidator()

for file in input_files:
    df = pd.read_csv(file)
    cleaned_df, report = validator.validate_and_clean(df, config)
    # Store results
```

---

## ğŸ“ˆ Key Metrics

The validator tracks:
- **Rows Processed**: Total input rows
- **Rows Cleaned**: Output rows after validation
- **Issues Found**: Count and types of quality issues
- **Processing Time**: Validation duration
- **Quality Score**: Percentage of valid records

---

## ğŸ§ª Testing

Run the validator on sample data:

```bash
python -c "
import pandas as pd
from data_quality_validator import DataQualityValidator

# Create sample data
df = pd.DataFrame({
    'user_id': [1, 1, 3],
    'age': [25, 30, 150],
    'email': ['user@example.com', 'invalid-email', 'another@domain.com']
})

config = {
    'check_duplicates': {'subset': ['user_id']},
    'range_checks': {'age': {'min': 0, 'max': 120}},
    'pattern_validation': {'email': {'pattern': r'^[^@]+@[^@]+\.[^@]+$'}}
}

validator = DataQualityValidator()
cleaned_df, report = validator.validate_and_clean(df, config)
print(cleaned_df)
"
```

---

## ğŸ“š Documentation

- **Technical Documentation**: See [data_quality_documentation.md](data_quality_documentation.md) for detailed architecture and design decisions
- **API Reference**: Check docstrings in `data_quality_validator.py`
- **Sample Reports**: View `quality_report.json` for example output format

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for enhancement:

- [ ] Additional outlier detection methods (Isolation Forest, Local Outlier Factor)
- [ ] Distribution-based anomaly detection
- [ ] Support for categorical data validation
- [ ] Machine learning-based data quality scoring
- [ ] Real-time validation streaming
- [ ] Web dashboard for quality metrics

---

## ğŸ“ License

This project is part of the Capstone Portfolio project.

---

## ğŸ™‹ Support & Questions

For issues, questions, or feature requests, please refer to the technical documentation or contact the development team.

---

## ğŸ“ Learning Outcomes

This project demonstrates:
- âœ“ Production-grade data validation patterns
- âœ“ Logging and observability best practices
- âœ“ Configurable framework design patterns
- âœ“ Error handling and data resilience
- âœ“ ETL/ELT pipeline integration
- âœ“ Data quality metrics and monitoring

---

**Last Updated**: February 2026  
**Status**: Production Ready
